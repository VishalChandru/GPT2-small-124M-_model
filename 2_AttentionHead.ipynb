{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "weights_q = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "weights_k = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "weights_v = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs @ weights_q\n",
    "keys = inputs @ weights_k\n",
    "value = inputs @ weights_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1484, 0.2285, 0.2217, 0.1301, 0.0883, 0.1831],\n",
       "        [0.1401, 0.2507, 0.2406, 0.1157, 0.0687, 0.1842],\n",
       "        [0.1406, 0.2496, 0.2397, 0.1164, 0.0696, 0.1841],\n",
       "        [0.1548, 0.2130, 0.2083, 0.1394, 0.1047, 0.1799],\n",
       "        [0.1577, 0.2067, 0.2028, 0.1428, 0.1122, 0.1777],\n",
       "        [0.1494, 0.2267, 0.2202, 0.1310, 0.0901, 0.1825]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score = query @ keys.T\n",
    "attention_weights = torch.softmax(attention_score, dim=-1)\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3071, 0.8230],\n",
       "        [0.3157, 0.8430],\n",
       "        [0.3152, 0.8421],\n",
       "        [0.3006, 0.8080],\n",
       "        [0.2978, 0.8016],\n",
       "        [0.3063, 0.8214]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector = attention_weights @ value\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=2, bias=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(d_in,d_out, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5337, -0.1051],\n",
       "        [-0.5323, -0.1080],\n",
       "        [-0.5323, -0.1079],\n",
       "        [-0.5297, -0.1076],\n",
       "        [-0.5311, -0.1066],\n",
       "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self,d_in,d_out,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.weights_Q = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "        self.weights_K = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "        self.weights_V = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        query = self.weights_Q(x)\n",
    "        key = self.weights_K(x)\n",
    "        value = self.weights_V(x)\n",
    "\n",
    "        attention_score = query @ key.T\n",
    "        attention_weight = torch.softmax(attention_score/ (key.shape[1] ** 0.5), dim=-1)\n",
    "\n",
    "        context_vector = attention_weight @ value\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa = SelfAttention(3,2)\n",
    "sa(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3821,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.2745, 0.6584,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.9268, 0.7388, 0.7179,   -inf,   -inf,   -inf],\n",
      "        [0.0772, 0.3565, 0.1479, 0.5331,   -inf,   -inf],\n",
      "        [0.4545, 0.9737, 0.4606, 0.5159, 0.4220,   -inf],\n",
      "        [0.9455, 0.8057, 0.6775, 0.6087, 0.6179, 0.6932]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones([6,6]), diagonal=1)\n",
    "print(torch.rand([6,6]).masked_fill(mask.bool(), -torch.inf))\n",
    "mask.bool()[:4][:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Attention (Masked-Self Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3325, -0.1223],\n",
       "         [-0.5163, -0.1861],\n",
       "         [-0.3971, -0.1450],\n",
       "         [-0.4687, -0.1633]],\n",
       "\n",
       "        [[-0.1982, -0.1163],\n",
       "         [-0.3748, -0.1848],\n",
       "         [-0.4641, -0.2301],\n",
       "         [-0.5462, -0.2600]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self,d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.weights_Q = nn.Linear(d_in,d_out, bias = qkv_bias)\n",
    "        self.weights_K = nn.Linear(d_in,d_out, bias = qkv_bias)\n",
    "        self.weights_V = nn.Linear(d_in,d_out, bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length,context_length), diagonal=1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch_size, context_length, emb_dim = x.shape\n",
    "        query = self.weights_Q(x)\n",
    "        key = self.weights_K(x)\n",
    "        value = self.weights_V(x)\n",
    "\n",
    "        attention_score = query @ key.transpose(1,2)\n",
    "        attention_score = attention_score.masked_fill(mask.bool()[:context_length,:context_length], -torch.inf)\n",
    "        attention_weights = torch.softmax(attention_score/ key.shape[1] ** 2, dim = -1)\n",
    "\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        context_vector = attention_weights @ value\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "torch.manual_seed(123)\n",
    "inputs = torch.rand([2,4,3])\n",
    "context_length = inputs.shape[1]\n",
    "ca = CausalAttention(d_in = 3,d_out = 2, context_length=context_length, dropout = 0.0)\n",
    "ca(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead Attention Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3863,  0.1636, -0.2143,  0.4306],\n",
      "         [ 0.4723,  0.2268, -0.4003,  0.4802],\n",
      "         [ 0.3628,  0.1752, -0.3094,  0.3714],\n",
      "         [ 0.4285,  0.2032, -0.3585,  0.4283],\n",
      "         [ 0.3775,  0.1839, -0.3268,  0.3890],\n",
      "         [ 0.3842,  0.1951, -0.3605,  0.4010]],\n",
      "\n",
      "        [[ 0.6047,  0.3282, -0.5738,  0.7392],\n",
      "         [ 0.6680,  0.3543, -0.6236,  0.7846],\n",
      "         [ 0.5888,  0.2982, -0.4944,  0.6919],\n",
      "         [ 0.6332,  0.2833, -0.4200,  0.6942],\n",
      "         [ 0.6835,  0.3021, -0.4513,  0.7311],\n",
      "         [ 0.6988,  0.3034, -0.4423,  0.7439]]], grad_fn=<CatBackward0>)\n",
      "torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in,d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias=qkv_bias) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim = -1)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "inputs = torch.rand([2,6,3])\n",
    "context_length = inputs.shape[1]\n",
    "mha = MultiHeadAttentionWrapper(d_in = 3,d_out = 2, context_length=context_length, dropout = 0.0, num_heads = 2)\n",
    "context_vector = mha(inputs)\n",
    "print(context_vector)\n",
    "print(context_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead Atttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4856, -0.5293, -0.3424,  0.0471],\n",
       "         [ 0.4893, -0.5318, -0.3472,  0.0396],\n",
       "         [ 0.4861, -0.5299, -0.3431,  0.0458],\n",
       "         [ 0.4882, -0.5310, -0.3457,  0.0420]],\n",
       "\n",
       "        [[ 0.6483, -0.5114, -0.4756, -0.0350],\n",
       "         [ 0.6530, -0.5127, -0.4810, -0.0411],\n",
       "         [ 0.6525, -0.5113, -0.4799, -0.0383],\n",
       "         [ 0.6543, -0.5116, -0.4819, -0.0403]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.d_out = d_out\n",
    "\n",
    "        self.weights_Q = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "        self.weights_K = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "        self.weights_V = nn.Linear(d_in,d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out,d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size, context_length, emb_dim = x.shape\n",
    "\n",
    "        query = self.weights_Q(x)\n",
    "        key = self.weights_K(x)\n",
    "        value = self.weights_V(x)\n",
    "\n",
    "        query = query.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "        key = key.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "        value = value.view(batch_size, context_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        query = query.transpose(1,2)\n",
    "        key = key.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "\n",
    "        attention_score = query @ key.transpose(2,3)\n",
    "        \n",
    "        attention_score.masked_fill(self.mask.bool()[:context_length, :context_length], -torch.inf)\n",
    "\n",
    "        attention_weight = torch.softmax(attention_score/key.shape[-1]**0.5, dim=-1)\n",
    "        self.dropout(attention_weight)\n",
    "\n",
    "        context_vector = (attention_weight @ value).transpose(1,2)\n",
    "\n",
    "        context_vector = context_vector.contiguous().view(batch_size, context_length, self.d_out)\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.rand([2,4,3])\n",
    "context_length = inputs.shape[1]\n",
    "mha = MultiHeadAttention(d_in=3, d_out=4, num_heads=2, dropout=0.0, context_length=context_length)\n",
    "mha(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
